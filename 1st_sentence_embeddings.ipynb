{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1st_sentence_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3f8j1oM4oHf"
      },
      "source": [
        "# ***Alexanda Apostolopoulou***\n",
        "# ***1115201700005***\n",
        "# *Project 4 in AI (II)* "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKbImGEUiv7c"
      },
      "source": [
        "# 1st Model of Sentence Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7_XjgUqJfs9"
      },
      "source": [
        "Import some useful libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OLeQyU4nNA-",
        "outputId": "b01f71e8-bfd9-4d81-d313-a45be335e192"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import glob\n",
        "import json  \n",
        "from pandas.io.json import json_normalize  \n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "!pip install spicy\n",
        "import scipy.spatial\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spicy\n",
            "  Downloading https://files.pythonhosted.org/packages/10/f7/58fd43678e56f6eed4b4a186dba367be3b56f95bb5a15741a0bf9861dde4/spicy-0.16.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from spicy) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->spicy) (1.19.5)\n",
            "Installing collected packages: spicy\n",
            "Successfully installed spicy-0.16.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPv3nPvlv6RE"
      },
      "source": [
        "####***Note:*** I implement both questions 1 and 2 in the same notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpa4LXrqJkc-"
      },
      "source": [
        "## ***Step 1:*** Download and Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cA01XWKJtyv"
      },
      "source": [
        "Here I download the initial dataset of COVID-19 Open Research Dataset (CORD-19). Since the dataset is too big, I couldn't upload it to my Drive as in previous Projects, because it's take several hours and it needed to allocate much of my memory. So I decide to download every dataset I would like to experiment with. As you can see I experimented with some different datasets, but I didn't see any improvement (I will refer to this later). The code above is in comments, because after I read the useful data I want from specific files, I saved them to my drive, so it doesn't have to do all this preprocess every time from scratch. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90Lk8rbLMSPG"
      },
      "source": [
        "Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZm6VFLFTj8H"
      },
      "source": [
        "# !wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-03-13.tar.gz\n",
        "# !wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-03-20.tar.gz\n",
        "# !wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-05-28.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmCs8Q5XMWK2"
      },
      "source": [
        "Extract the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfzCFwpjlQGU"
      },
      "source": [
        "# !tar -xf cord-19_2020-03-13.tar.gz\n",
        "# !tar -xf cord-19_2020-03-20.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1M21TiZNqH2"
      },
      "source": [
        "Go to the specific folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PoF2XjglqzT"
      },
      "source": [
        "# %cd /content/2020-03-13\n",
        "# %cd /content/2020-03-20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG8ADqgtN0B8"
      },
      "source": [
        "Extract the desirable folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeWVZsDtm3ni"
      },
      "source": [
        "# !tar -xf comm_use_subset.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTMvVCqBOXfq"
      },
      "source": [
        "####Here I take only the title, the abstract and the body texts of each paper and I save them to list of lists. \n",
        "\n",
        "####I chose only this three parts of information, because I believe that the sections of paper id, authors, publish_time, url, source, license etc, are irrelevant with the answer I was looking for and they only add more complexity with no reason. \n",
        "\n",
        "####In order to understand the structure of my data, the element data[0] is refered to the first article, data[0][0] is refered to the title of the first aricle, data[0][1] to the abstract of the first article and element data[0][2] to the whole body text content of the first paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6gftCmLNOvs"
      },
      "source": [
        "# data = []\n",
        "\n",
        "# files = glob.glob('comm_use_subset/*', recursive=True)\n",
        "\n",
        "# for single_file in files:\n",
        "#   with open(single_file, 'r') as f:\n",
        "#     d = json.load(f) \n",
        "\n",
        "#     df = pd.json_normalize(d)\n",
        "\n",
        "#     title = df['metadata.title'].item()\n",
        "\n",
        "#     if df['abstract'].item() == []:\n",
        "#       abstract = df['abstract'].item()\n",
        "#     else:\n",
        "#       abstract_data = pd.json_normalize(data = d, record_path ='abstract') \n",
        "#       if (abstract_data['text'].shape[0] > 1):\n",
        "#         abstract = abstract_data['text'].str.cat(sep='. ')\n",
        "#       else:\n",
        "#         abstract = abstract_data['text'].item()\n",
        "\n",
        "#     text_data = pd.json_normalize(data = d, record_path ='body_text') \n",
        "\n",
        "#     if (text_data['text'].shape[0] > 1):\n",
        "#       body_text = text_data['text'].str.cat(sep='. ')\n",
        "#     else:\n",
        "#       body_text = text_data['text'].item()\n",
        "\n",
        "#     data.append([title,abstract,body_list])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9meJdR1V-eL"
      },
      "source": [
        "# print(len(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1VtjWQLi-38"
      },
      "source": [
        "# print(data[0])\n",
        "# print('\\n')\n",
        "# print(data[0][0])\n",
        "# print('\\n')\n",
        "# print(data[0][1])\n",
        "# print('\\n')\n",
        "# print(data[0][2])\n",
        "# print(len(data[0][2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCslLI5PP_v_"
      },
      "source": [
        "Save the previous data locally and then in my Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip_XYYzbn420"
      },
      "source": [
        "# import os\n",
        "# cwd = os.getcwd()\n",
        "# print(cwd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWzufphinSlB"
      },
      "source": [
        "# with open('listfile.data', 'wb') as filehandle:\n",
        "#     # store the data as binary data stream\n",
        "#     pickle.dump(data, filehandle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjpV8EyCoGWC"
      },
      "source": [
        "# with open('listfile.data', 'rb') as filehandle:\n",
        "#     # read the data as binary data stream\n",
        "#     articles = pickle.load(filehandle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1NxVnHvoPkP"
      },
      "source": [
        "# print(len(articles))\n",
        "# print(articles[0])\n",
        "# print('\\n')\n",
        "# print(articles[0][0])\n",
        "# print('\\n')\n",
        "# print(articles[0][1])\n",
        "# print('\\n')\n",
        "# print(articles[0][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJc3BFCyuTDt"
      },
      "source": [
        "# import shutil\n",
        "# shutil.move('/content/listfile_3.data', '/content/drive/MyDrive/listfile_3.data')\n",
        "# # shutil.move('/content/listfile_2.data', '/content/drive/MyDrive/listfile_2.data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAjdF-KFQFva"
      },
      "source": [
        "## ***Step 2:*** Load my preprocessed data from my Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzKhXX00ujwN"
      },
      "source": [
        "with open('/content/drive/MyDrive/listfile.data', 'rb') as filehandle:\n",
        "    # read the data as binary data stream\n",
        "    articles = pickle.load(filehandle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFv0M4SQXqC"
      },
      "source": [
        "Check if everything is ok"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75ULukVcvENA",
        "outputId": "355c3458-12f3-4fa5-f4e2-0aa52cd831c0"
      },
      "source": [
        "# Print the number of articles\n",
        "print(len(articles))\n",
        "# Print the first article\n",
        "print(articles[0])\n",
        "print('\\n')\n",
        "# Print the first title \n",
        "print(articles[0][0])\n",
        "print('\\n')\n",
        "# Print the first abstract \n",
        "print(articles[0][1])\n",
        "print('\\n')\n",
        "# Print the first body text \n",
        "print(articles[0][2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9000\n",
            "['ethical and Philosophical considerations for Gain-of-Function Policy: the importance of Alternate experiments The Department of Health and Human Services Framework for Guiding Funding Decisions about Proposed Research Involving Enhanced Potential Pandemic Pathogens', 'PPPs) contains a series of principles for governing the funding and conduct of gain-offunction (GOF) research resulting in the creation of PPPs. In this article, I address one of these principles, governing the replacement of GOF research with alternate experiments. I argue that the principle fails to address the way that different experiments can promote the same values as those promoted by GOF research resulting in PPPs. I then address some objections to this claim, and provide policy recommendations moving forward.', 'Concerns over the accidental or deliberate release of novel pathogens has prompted a debate about the conduct or funding of biological research in the name of human health and security. Of particular concern is \"gain-of-function\" (GOF) research resulting in the creation of a \"potential pandemic pathogen (PPP)\" (GOF/PPP research) where, inter alia, the host range, virulence, or transmissibility of a pathogen is enhanced. For example, in 2011 researchers modified highly pathogenic avian influenza (HPAI) H5N1 to transmit via respiratory droplets in mammals (Herfst et al., 2012; Imai et al., 2012) .. The 2011 HPAI H5N1 studies, and indeed all GOF/PPP research, generate biosafety and biosecurity concerns. 1 Initial concern over the 2011 HPAI H5N1 studies focused on biosecurity: the risk of a deliberate release of an organism through, e.g., the use of a biological weapon (Evans, 2013) . This concern evolved to encompass biosafety concerns that an accidental release of recombinant influenza could cause a global pandemic (Lipsitch and Galvani, 2014; Lipsitch and Inglesby, 2014; Evans et al., 2015) .. In 2014, in response to these concerns, the US Government imposed a funding pause of GOF research involving influenza, severe acute respiratory syndrome (SARS) coronavirus, and Middle East respiratory syndrome (MERS) coronavirus (The White House, 2014). The pause was accompanied by a deliberative process undertaken by the US National Academies of Sciences, Engineering and Medicine (2016) , National Research Council Institute of Medicine (2015) , and National Science Advisory Board for Biosecurity (2016) to develop policy that managed the risks and Box 1 | Policy principles.. Agency review mechanisms pursuant to this recommended policy guidance should establish that a project involving the creation, transfer, or use of enhanced PPPs should satisfy the following principles, which are based on similar principles in the NSABB.. Recommendations:. 3.1. The proposal or plan for such a project has been evaluated by an independent expert review process (whether internal or external) and has been determined to be scientifically sound. 3.2. The pathogen that is anticipated to be generated by the project must be reasonably judged to be a credible source of a potential future human pandemic. 3.3. An assessment of the overall potential risks and benefits associated with the project determines that the potential risks as compared to the potential benefits to society are justified. 3.4. There are no feasible, equally efficacious alternative methods to address the same question in a manner that poses less risk than does the proposed approach. 3.5. The investigator and the institution where the project would be carried out have the demonstrated capacity and commitment to conduct it safely and securely and have the ability to respond rapidly, mitigate potential risks and take corrective actions in response to laboratory accidents, lapses in protocol and procedures, and potential security breaches. 3.6. The project\\'s results are anticipated to be responsibly communicated, in compliance with applicable laws, regulations, and policies, and any terms and conditions of funding, in order to realize their potential benefit. 3.7. The project will be supported through funding mechanisms that allow for appropriate management of risks and ongoing Federal and institutional oversight of all aspects of the research throughout the course of the project. 3.8. The project is ethically justifiable. Non-maleficence, beneficence, justice, respect for persons, scientific freedom, and responsible stewardship are among the ethical values that should be considered by a multidisciplinary review process making decisions about whether to fund research involving PPPs (Office of Science and Technology Policy, 2017). Pathogens, marking an end to the funding pause. A diagnostic for the success of this policy process, and indeed a mark of progress after 6 years of debate about GOF/PPP research 2 would be to ask \"does this policy adequately select between GOF/ PPP research that on balance advances scientific knowledge and human values, and research that poses unacceptable risks to human health and security?\" Here, I argue the answer to this question is \"no. \" The P3CO principles fail to select between GOF/ PPP experiments by failing to consider that alternative experiments may provide a superior expected net benefit. In doing so, the policy fails to account for a range of important opportunities to enhance the safety and security of the life sciences without unduly burdening researchers or policymakers, or sacrificing meaningful progress in the life sciences.. In what follows, I describe the P3CO principles and their relationship to GOF/PPP research. I then argue that the principle suggesting that GOF/PPP research is justified if \"no feasible, equally efficacious alternative methods to address the same question in a manner that poses less risk than does the proposed approach\" is inappropriately permissive. I consider a series of objections to my account, and argue that none are sufficiently strong to justify the principle as it stands. I conclude with an alternate framing of these principles in the context of the DHHS policy.. The P3CO principles echo the Recommendations\\' guidance on federal department pre-funding review and approval of GOF/ PPP research. These principles apply to \"Gain of Function Research of Concern\" (GOFROC): GOF research that is (1) highly transmissible and likely capable of wide and uncontrollable spread in human populations and (2) highly virulent and likely to cause significant morbidity and/or mortality in humans (National Science Advisory Board for Biosecurity, 2016). Recommendations, P3CO, and the DHHS policy note that all principles must be satisfied in order to pursue GOFROC. The document also suggests other risk mitigation strategies should biosafety and/or security issues arise post-funding, including measures should the final publication of results pose an information risk. I have discussed information risk posed by GOF/PPP research-of which I consider GOFROC a subset 3 -elsewhere (Evans, 2013; Evans and Selgelid, 2014) , and set this aside for the purpose of my analysis.. P3CO and the final HHS policy outline the following principles for the funding and conduct of GOFROC. First, GOFROC must be subject to independent review to determine it answers a scientifically sound question. While the NSABB intimated in their articulation of this principle that this review should ideally be supplemental to existing review mechanisms for funded research, the P3CO principles remain silent on whether existing grant review mechanisms (which presumably select for scientifically sound projects) satisfy this principle.. Second, pathogens anticipated to be created by projects subject to the policy must be a credible source of a potential future human pandemic. Moreover, the experiments must also be assessed to provide benefits that outweigh the risks of conducting GOFROC. These principles respond to general concern that some GOF/PPP research presents risks that ultimately outweigh its potential benefits. The 2011 HPAI H5N1 papers, for example, were published after the NSABB reviewed revised drafts of the papers, which defended the public health benefits of the research to respond to an emerging HPAI H5N1 pandemic (Enserink, 2012) . Interestingly, the language between Recommendations and P3CO changes significantly regarding the second principle above: the former requires that the pathogen be expected to \"arise through natural processes, \" while the latter merely requires a \"credible\" source, opening the possibility that GOFROC research might be funded to address human-engineered PPPs.. My analysis will focus on the fourth principle, which is that research should be funded only if \"there is no feasible, equally efficacious alternative method to address the same question in a manner that poses less risk than does the proposed [GOFROC] approach. \" The Recommendations add that Alternative approaches must be explored and critically examined before funding [GOFROC]… modifications of the experimental design, use of attenuated or other strains that pose fewer risks to humans, or different approaches with less risk that may provide the same or very similar information may be feasible. Lines of experimentation that entail less risk should be pursued whenever possible (National Science Advisory Board for Biosecurity, 2016, 45).. The next three principles are procedural components on GOFROC funding. First, researchers and their institutions must have sufficient capacity to mitigate the risks of the research. While paradigm cases of GOF/PPP research occurred within large and expensive, high-containment laboratories, the increasing power of the life scientists mean that resource constraints on GOF/PPP research will ease in the long term. The research must also be communicated in compliance with applicable laws and regulations, and \"supported through funding mechanisms that allow for appropriate management of risks and ongoing Federal and institutional oversight of all aspects of the research throughout the course of the project. \" That is, the funding mechanism under which these projects are conducted ought to be flexible enough to accommodate changes to methodology, or additional resource requirements for risk mitigation.. Finally, the principles require the project be ethically justifiable, citing \"values\" 4 such as non-maleficence, beneficence, justice, respect for persons, scientific freedom, and responsible stewardship. While P3CO does not articulate what these mean 4 I place values in quotes because it isn\\'t immediately clear what kind of values these are. The first four are drawn from Beauchamp and Childress, who articulate them as mid-level principles that point at a range of fundamental values, but do not depend on our strict commitment to particular values for us to agree to their importance (Beauchamp and Childress, 2012) . For example, respect for persons is derived from a (primarily negative) conception of autonomy, that is, being able to pursuing one\\'s own projects without interference by others. Scientific freedom, on the other hand, need not be a \"value\" per se, but may be valuable as a component of the right to freedom of speech (Evans, 2013) or freedom of inquiry (Miller and Selgelid, 2008) . I won\\'t inquire more into these ethical principles, except to note that their status as values is subject to considerable debate. for the governance of GOFROC, the Recommendations reference an ethical white paper produced during the deliberative process (Selgelid, 2016) .. A critical question is what these principles might permit or exclude. The NSABB acknowledged some GOF/PPP research (or GOFROC) might be too risky to fund or conduct, but did not give a specific example of what such an experiment might entail (National Science Advisory Board for Biosecurity, 2016, p. 4). Bearing in mind that GOFROC must meet all of these criteria some research is clearly excluded from funding: research that does not ask a scientifically sound question, does not address a credible source of human pandemic, or whose expected benefits are clearly outweighed by its expected risk. Moreover, in cases where a researcher or institution is unable to comply with federal laws and regulations, or ensure a safe research environment in the context of GOFROC, funding ought not be given for such experiments.. When it comes to the fourth principle-that the research cannot be feasibly or efficaciously pursued through another methodology that answers the same scientific question-it isn\\'t clear that any experiment would fail to satisfy this criterion. Methodologies in the life science closely track the organization of the field, its priorities, and the kinds of question that are asked. In a very simple sense, different methodologies, even if they create similar (or similarly valuable) knowledge, do not ask \"the same question. \" To change methodology is, in a very basic but important sense, to change the question.. This principle highlights, moreover, a strong division in the debate over GOF/PPP research. Advocates for GOF/PPP research have argued that GOF as a methodology has unique epistemic merits. While other experiments may allow us to demonstrate the potential for a pathogen to alter its host range or experience enhanced transmissibility or virulence, advocates maintain that only GOF can show us this is possible. As such, a change to an alternate methodology deprives us of the one methodological tool we have to conclusively prove that a (wild-type) virus can acquire the potential to cause a human pandemic (Casadevall et al., 2014) .. It is thus difficult to understand, then, what work is done by the fourth principle as it is written. One suggested case in which an alternate methodology might answer the same question would be one in which a specific part of a pathogen\\'s structure altered to demonstrate a change in function. 5 Say, for example, we wanted to determine whether a virus with a substituted HA protein would bind to human receptors. It would be possible to answer this question using an attenuated virus rather than its wild-type progenitor. A more radical alternative might be cell-free study of single proteins-for example, H5 or H7 receptor binding to mammalian sialic acids-could take the place of GOF/PPP research with similar aims (Lipsitch and Galvani, 2014) and eliminate the need for a live virus (at least, in initial research). 6 This a rare example, however, and does not represent paradigm GOF/PPP studies studying which mutations in a viral genome are attributed to changes in phenotype under laboratory conditions. Alternate experiments to these GOF/PPP experiments may answer-related questions (Lipsitch and Galvani, 2014) and have considerable scientific and public health value, but do not answer precisely the same question. They are thus excluded as potential replacements for GOF/PPP studies under the new policy.. Yet the existence of alternate experiments, and the serious risks presented by some GOF/PPP research gives us a pro tanto justification for substituting experiments even if they do not ask the same question. This justification is particularly compelling if alternate experiments advance therapeutics development, disease surveillance, or public health response to emerging pandemics. Next, I\\'ll consider three objections that might arise from those who may wish to retain the current formulation of the fourth principle.. Here, I outline four potential objections and respond to them. These are:. 1. Assert the P3CO principles are appropriate in the context of existing scientific governance; 2. Beyond instrumental benefits, scientists should be free to choose which questions they ask; 3. Appeal to the NSABB Recommendations for guidance on \"same or similar\" questions; 4. Appropriately tailoring the risk-benefit difference in principle three will select against the right set of experiments.. I respond to these in turn.. One could contend these P3CO principles are appropriate in the context of existing scientific governance. That is, as long as the expected risk-benefit difference is positive (i.e., sufficiently in favor of benefit), the (scientifically meritorious) research ought to proceed. We ought not be in the business of guessing whether other questions might give equally valuable answers, or trying as policymakers to guess which scientific question is more worthy of pursuing.. Advocates typically justify GOF/PPP experiments on their role in achieving some other end, e.g., developing novel vaccines or therapeutics (Schultz-Cherry et al., 2014), or enhancing disease surveillance (Casadevall et al., 2014) . There is some value in GOF/ PPP research for its own sake, but this is the case for any scientifically meritorious question. Moreover, the value of preventing an accidental or deliberately caused disease pandemic arguably outweighs the mere value of scientific knowledge for its own sake. Advocates of GOF/PPP don\\'t just leverage the instrumental value of these studies in their argument; they rely on them.. Yet funders prioritize those ends, and thus partly determine what scientific questions can be asked; we already do what advocates of GOF/PPP research deny. Allowing similar, but 6 Thanks to an anonymous reviewer for this suggestion. not identical questions to take the place of GOF/PPP research amounts a small change in degree from existing practice. Only occasionally is GOF/PPP research a unique means to medical and public health ends (Evans, 2014) ; other experiments may answer different scientific questions, but still answer questions that achieve the same ends as GOF/PPP research. The GOF benefit analysis conducted during the deliberative process demonstrated that only in 9 of 24 scientific and public health goals addressed for influenza was GOF/PPP uniquely useful; this number was only 3 in 13 for coronavirus (SARS and MERS) research (Gryphon Scientific, 2016, pp. 249-254) .. A proponent of GOF/PPP research might reply that what matters isn\\'t (only) that we achieve our scientific or public health aims, but that scientists are also free to choose which questions they ask. Scientific freedom, after all, is not simply an efficient means to achieve material ends, but valuable for its own sake (Evans, 2013) . This response also fails: scientific freedom is important, but we already acknowledge limits on that freedom given certain risks to others (e.g., in human experimentation). Given that the set of experiments we can permissibly fund-and thus, the questions we ask-is already much smaller than the total set of scientific questions we have available to ask, it seems minimally invasive to ask that where possible, we pick from a set of related questions that accomplish our aims while entailing fewer, or less extreme, risks.. One might appeal to the Recommendations for guidance. In their fourth recommendation, the NSABB notes that review of GOFROC should include consideration of alternate methodologies that provide the \"same or very similar information\" at lower risk. This presumably a larger set than methodologies that strictly answer the same scientific question. How large a set this is, however, depends on what constitutes \"information\" in this sense, and what our basis for comparison determine similarity might be.. What might this conception of similarity look like? Consider an interesting piece of scientific knowledge such as \"the sequence changes required to confer mammalian transmissibility via respiratory droplets onto HPAI viruses (Watanabe et al., 2014) . \" GOF/PPP experiments, at least those that spurred the deliberative processes, ultimately provided one set of information that produces the kind of scientific knowledge we want. It did so, moreover, in a very specific way: by providing the exact sequence change required for specific isolates of HPAI. In the case of the 2011 experiments, these were sequence changes for influenza virus A/Indonesia/5/2005 (Herfst et al., 2012) , and six reassortant viruses from a library of reassortants from A/Vietnam/1203/2004 (H5N1) and A/Puerto Rico/8/34 (H1N1).. Critics have noted that while these experiments provide sequence changes required to confer mammalian transmissibility via respiratory droplets onto HPAI viruses, the sequence changes developed in laboratories may not be likely or even plausible in wild flu viruses (Enserink, 2011; Lipsitch and Galvani, 2014) . Other methodologies may provide likely sequence changes for a broader range of viruses, some (or more) of which are plausible in nature. This provides a similar kind of knowledge, albeit less precise (in the sense that it does not provide us with an exact sequence) but with a greater degree of generalization. Both give us our sought after knowledge in some sense of the initial parameters.. How we understand similarity depends, in part, on what kinds of knowledge we find valuable. Let us assume, plausibly, that the general scientific interest of each study is equal (though perhaps to different sets of scientists). Whether these two sets of experiment provide relevantly \"very similar\" information depends, again, on the relationship between our ends and the kinds of knowledge about which we are interested. If our interest is cataloging individual HPAI viruses for their capacity to undergo sustained transmission and are able to look for those specific sequences in nature, then GOF/PPP experiments occupy a fairly narrow space in which we could find other, similar experiments that generate similar information. If, however, we are interested in determining which kinds of sequence are most likely to appear in flu viruses in the wild, GOF/PPP experiments are arguably not unique and may even by counterproductive as a form of knowledge generation . How we define similarity depends on our ends; P3CO\\'s reduction from similarity to identity commits us to valuing scientific information in a way we might otherwise not endorse.. While I have used the 2011 HPAI H5N1 studies in my argument, other experiments may have different goals for which GOF/PPP methodologies are more suited. Studies involving MERS coronavirus, for example, were allowed to continue after the initial pause because their aim was to develop an animal model for MERS in which to conduct experiments (Kaiser, 2014) . Arguably, GOF/PPP research in aid of this pursuit may contribute uniquely to the development of an animal model in MERS or be so efficacious a method as to outweigh potential risks.. Finally, one might respond that the most crucial aspect here is the risk-benefit difference in principle three. That is, one might argue that the primary principle on which we ought to judge GOF/PPP research is whether its expected benefits outweigh its expected risks. Moreover, if we set our threshold for acceptable risk correctly, we will exclude those experiments that are too risky relative to their expected benefits and default to other methodologies. This is promising insofar as it forces us to specify exactly how much our purported benefits must exceed potential risks in order to justify funding GOF/PPP experiments. Yet it elides the comparative nature of risk assessment. Decisions under conditions of risk are, and ought to be, comparative (see, e.g., Hansson, 2003) . Say we believe the right decision is one that maximizes the probability-weighted net benefits of our actions. And let us also say a GOF/PPP experiment might lead to the development of a novel vaccine: say, a 90% chance, and would save 10 million people; but also entails an independent nontrivial \"global catastrophic risk\" of 0.01% of a disease pandemic that kills 1 billion people. Our probability adjusted net benefit is thus 8.9 million people: we expect, all things considered, that many more people will be saved than die as a result of our experiment. Now imagine an alternate experiment called alt-GOF. This experiment has the same probability of causing benefit or harm, but the magnitude of the benefit and harms are 9.9 million and 10 million, respectively. Our expected value is now 8.909 million lives: about 9,090 more than the initial case. It isn\\'t sufficient to say GOF leads to a positive net benefit. We have clear reason to prefer alt-GOF to GOF/PPP, regardless of whether or not it answers the exact same question. 7 Risk assessments ought to be understood as comparisons rather than absolute references (National Research Council Institute of Medicine, 2015) . It isn\\'t sufficient to say we are justified in funding GOF/PPP research just in case its expected benefits outweigh its expected risks. Rather, we want to say GOF/PPP provides the most justifiable tradeoff between benefit and risk. What that tradeoff ought to be is unclear from the P3CO policy. Regardless of what criteria we use for decision-making under risk, our principles for governance ought to be comparative. Principle four undercuts this by precluding a large set of options from which comparison can follow.. In response to my argument, the following solutions could be implemented individually or in concert.. (1) Clarify principle four in broader terms. HHS and other agencies could release additional guidance that would clarify principle four to include considerations of a reasonable range of alternative experiments with independent scientific merit that promote the same aims as GOF/PPP research. A move away from the \"same question\" or \"very similar information\" to a broader conception of alternatives would develop the governance of this research in important ways, critically in forcing policymakers and scientists to specify what, exactly, they wish GOF/PPP research to accomplish.. This is a plausible in the context of section V of the current HHS policy, which claims that \"HHS will periodically re-evaluate and modify this review process, as necessary, to reflect scientific advances and changes to the regulatory landscape\" (Department of Health and Human Services, 2017). We could envisage such a review tackling, first, the question of alternate experiments and building on existing work [e.g., the risk and benefit assessment from the deliberative process (Gryphon Scientific, 2016) ] to develop specific guidance on alternate experiments that fulfill a range of scientific and public health aims.. (2) Introduce comparative subprinciples into principle three.. That is, rather than attempt to realign principle four with the above critiques, further incorporate comparative risk assessment into the third principle and its translation into actionable policy. By explicitly noting that risk assessments must be comparative over a range of plausible alternatives within the broader aims of the funded research, agencies can side-step potential issues with principle four.. This recommendation would be a plausible alternative to my first recommendation by incorporating the substance of my critique into principle three, rather than principle four. It would conceivably render principle four unnecessary. Rather, the risk assessment would be comparative and account for alternatives that produce valuable scientific information that is both similar to and as valuable as proposed GOF/PPP research, but may entail fewer serious risks. This could be accomplished without a specific policy change in the context of the HHS policy, instead built into future procedural elements of the review process that have yet to be described. The risk here is that excluding principle four as an explicit consideration reopens the possibility that alternates are never taken seriously; I acknowledge this, but note that my above suggestion, taken in good faith, makes this unlikely.. (3) Fund comparative assessments of scientific research and alternatives to GOF/PPP. Requiring a risk and benefit analysis as part of policy is itself challenging, and other policies on dual-use research of concern have been criticized by practitioners for requiring analysis that is beyond the skills of any institutional oversight body. Rather than require de novo analysis for each GOF/PPP experiment to arise, research on these experiments-using the risk and benefit analysis from the deliberative process as a template-could develop a hierarchy of comparatively low-, medium-, and high-risk experiments, and the conditions under which they are uniquely justified or overwhelmingly efficacious. A kind of \"GOF/PPP case law\" is itself a valuable addition to the life sciences, and an important policy innovation.. We already have some precedent for this in the form of those MERS studies that were reinstated after the initial pause. This kind of research arguably provides us with an important data point on what counts as efficacious and justified GOF/PPP research looks like. In contrast, research that created mammalian transmissible variants of a virus that has never been recorded in mammals (Sutton et al., 2014) is arguably a high-risk, unjustified form of GOF/PPP research.. The new GOF/PPP principles require further interpretation to become effective governance. In this article, I have argued that principle four, \"the research cannot be feasibly or efficaciously pursued through another methodology that answers the same scientific question, \" is overly permissive and have suggested reform for policymakers. The debate over GOF/PPP research is surely not over, and whether these changes are ultimately incorporated is a matter for future policy work.. NE responsible for all research and writing.. reFereNces']\n",
            "\n",
            "\n",
            "ethical and Philosophical considerations for Gain-of-Function Policy: the importance of Alternate experiments The Department of Health and Human Services Framework for Guiding Funding Decisions about Proposed Research Involving Enhanced Potential Pandemic Pathogens\n",
            "\n",
            "\n",
            "PPPs) contains a series of principles for governing the funding and conduct of gain-offunction (GOF) research resulting in the creation of PPPs. In this article, I address one of these principles, governing the replacement of GOF research with alternate experiments. I argue that the principle fails to address the way that different experiments can promote the same values as those promoted by GOF research resulting in PPPs. I then address some objections to this claim, and provide policy recommendations moving forward.\n",
            "\n",
            "\n",
            "Concerns over the accidental or deliberate release of novel pathogens has prompted a debate about the conduct or funding of biological research in the name of human health and security. Of particular concern is \"gain-of-function\" (GOF) research resulting in the creation of a \"potential pandemic pathogen (PPP)\" (GOF/PPP research) where, inter alia, the host range, virulence, or transmissibility of a pathogen is enhanced. For example, in 2011 researchers modified highly pathogenic avian influenza (HPAI) H5N1 to transmit via respiratory droplets in mammals (Herfst et al., 2012; Imai et al., 2012) .. The 2011 HPAI H5N1 studies, and indeed all GOF/PPP research, generate biosafety and biosecurity concerns. 1 Initial concern over the 2011 HPAI H5N1 studies focused on biosecurity: the risk of a deliberate release of an organism through, e.g., the use of a biological weapon (Evans, 2013) . This concern evolved to encompass biosafety concerns that an accidental release of recombinant influenza could cause a global pandemic (Lipsitch and Galvani, 2014; Lipsitch and Inglesby, 2014; Evans et al., 2015) .. In 2014, in response to these concerns, the US Government imposed a funding pause of GOF research involving influenza, severe acute respiratory syndrome (SARS) coronavirus, and Middle East respiratory syndrome (MERS) coronavirus (The White House, 2014). The pause was accompanied by a deliberative process undertaken by the US National Academies of Sciences, Engineering and Medicine (2016) , National Research Council Institute of Medicine (2015) , and National Science Advisory Board for Biosecurity (2016) to develop policy that managed the risks and Box 1 | Policy principles.. Agency review mechanisms pursuant to this recommended policy guidance should establish that a project involving the creation, transfer, or use of enhanced PPPs should satisfy the following principles, which are based on similar principles in the NSABB.. Recommendations:. 3.1. The proposal or plan for such a project has been evaluated by an independent expert review process (whether internal or external) and has been determined to be scientifically sound. 3.2. The pathogen that is anticipated to be generated by the project must be reasonably judged to be a credible source of a potential future human pandemic. 3.3. An assessment of the overall potential risks and benefits associated with the project determines that the potential risks as compared to the potential benefits to society are justified. 3.4. There are no feasible, equally efficacious alternative methods to address the same question in a manner that poses less risk than does the proposed approach. 3.5. The investigator and the institution where the project would be carried out have the demonstrated capacity and commitment to conduct it safely and securely and have the ability to respond rapidly, mitigate potential risks and take corrective actions in response to laboratory accidents, lapses in protocol and procedures, and potential security breaches. 3.6. The project's results are anticipated to be responsibly communicated, in compliance with applicable laws, regulations, and policies, and any terms and conditions of funding, in order to realize their potential benefit. 3.7. The project will be supported through funding mechanisms that allow for appropriate management of risks and ongoing Federal and institutional oversight of all aspects of the research throughout the course of the project. 3.8. The project is ethically justifiable. Non-maleficence, beneficence, justice, respect for persons, scientific freedom, and responsible stewardship are among the ethical values that should be considered by a multidisciplinary review process making decisions about whether to fund research involving PPPs (Office of Science and Technology Policy, 2017). Pathogens, marking an end to the funding pause. A diagnostic for the success of this policy process, and indeed a mark of progress after 6 years of debate about GOF/PPP research 2 would be to ask \"does this policy adequately select between GOF/ PPP research that on balance advances scientific knowledge and human values, and research that poses unacceptable risks to human health and security?\" Here, I argue the answer to this question is \"no. \" The P3CO principles fail to select between GOF/ PPP experiments by failing to consider that alternative experiments may provide a superior expected net benefit. In doing so, the policy fails to account for a range of important opportunities to enhance the safety and security of the life sciences without unduly burdening researchers or policymakers, or sacrificing meaningful progress in the life sciences.. In what follows, I describe the P3CO principles and their relationship to GOF/PPP research. I then argue that the principle suggesting that GOF/PPP research is justified if \"no feasible, equally efficacious alternative methods to address the same question in a manner that poses less risk than does the proposed approach\" is inappropriately permissive. I consider a series of objections to my account, and argue that none are sufficiently strong to justify the principle as it stands. I conclude with an alternate framing of these principles in the context of the DHHS policy.. The P3CO principles echo the Recommendations' guidance on federal department pre-funding review and approval of GOF/ PPP research. These principles apply to \"Gain of Function Research of Concern\" (GOFROC): GOF research that is (1) highly transmissible and likely capable of wide and uncontrollable spread in human populations and (2) highly virulent and likely to cause significant morbidity and/or mortality in humans (National Science Advisory Board for Biosecurity, 2016). Recommendations, P3CO, and the DHHS policy note that all principles must be satisfied in order to pursue GOFROC. The document also suggests other risk mitigation strategies should biosafety and/or security issues arise post-funding, including measures should the final publication of results pose an information risk. I have discussed information risk posed by GOF/PPP research-of which I consider GOFROC a subset 3 -elsewhere (Evans, 2013; Evans and Selgelid, 2014) , and set this aside for the purpose of my analysis.. P3CO and the final HHS policy outline the following principles for the funding and conduct of GOFROC. First, GOFROC must be subject to independent review to determine it answers a scientifically sound question. While the NSABB intimated in their articulation of this principle that this review should ideally be supplemental to existing review mechanisms for funded research, the P3CO principles remain silent on whether existing grant review mechanisms (which presumably select for scientifically sound projects) satisfy this principle.. Second, pathogens anticipated to be created by projects subject to the policy must be a credible source of a potential future human pandemic. Moreover, the experiments must also be assessed to provide benefits that outweigh the risks of conducting GOFROC. These principles respond to general concern that some GOF/PPP research presents risks that ultimately outweigh its potential benefits. The 2011 HPAI H5N1 papers, for example, were published after the NSABB reviewed revised drafts of the papers, which defended the public health benefits of the research to respond to an emerging HPAI H5N1 pandemic (Enserink, 2012) . Interestingly, the language between Recommendations and P3CO changes significantly regarding the second principle above: the former requires that the pathogen be expected to \"arise through natural processes, \" while the latter merely requires a \"credible\" source, opening the possibility that GOFROC research might be funded to address human-engineered PPPs.. My analysis will focus on the fourth principle, which is that research should be funded only if \"there is no feasible, equally efficacious alternative method to address the same question in a manner that poses less risk than does the proposed [GOFROC] approach. \" The Recommendations add that Alternative approaches must be explored and critically examined before funding [GOFROC]… modifications of the experimental design, use of attenuated or other strains that pose fewer risks to humans, or different approaches with less risk that may provide the same or very similar information may be feasible. Lines of experimentation that entail less risk should be pursued whenever possible (National Science Advisory Board for Biosecurity, 2016, 45).. The next three principles are procedural components on GOFROC funding. First, researchers and their institutions must have sufficient capacity to mitigate the risks of the research. While paradigm cases of GOF/PPP research occurred within large and expensive, high-containment laboratories, the increasing power of the life scientists mean that resource constraints on GOF/PPP research will ease in the long term. The research must also be communicated in compliance with applicable laws and regulations, and \"supported through funding mechanisms that allow for appropriate management of risks and ongoing Federal and institutional oversight of all aspects of the research throughout the course of the project. \" That is, the funding mechanism under which these projects are conducted ought to be flexible enough to accommodate changes to methodology, or additional resource requirements for risk mitigation.. Finally, the principles require the project be ethically justifiable, citing \"values\" 4 such as non-maleficence, beneficence, justice, respect for persons, scientific freedom, and responsible stewardship. While P3CO does not articulate what these mean 4 I place values in quotes because it isn't immediately clear what kind of values these are. The first four are drawn from Beauchamp and Childress, who articulate them as mid-level principles that point at a range of fundamental values, but do not depend on our strict commitment to particular values for us to agree to their importance (Beauchamp and Childress, 2012) . For example, respect for persons is derived from a (primarily negative) conception of autonomy, that is, being able to pursuing one's own projects without interference by others. Scientific freedom, on the other hand, need not be a \"value\" per se, but may be valuable as a component of the right to freedom of speech (Evans, 2013) or freedom of inquiry (Miller and Selgelid, 2008) . I won't inquire more into these ethical principles, except to note that their status as values is subject to considerable debate. for the governance of GOFROC, the Recommendations reference an ethical white paper produced during the deliberative process (Selgelid, 2016) .. A critical question is what these principles might permit or exclude. The NSABB acknowledged some GOF/PPP research (or GOFROC) might be too risky to fund or conduct, but did not give a specific example of what such an experiment might entail (National Science Advisory Board for Biosecurity, 2016, p. 4). Bearing in mind that GOFROC must meet all of these criteria some research is clearly excluded from funding: research that does not ask a scientifically sound question, does not address a credible source of human pandemic, or whose expected benefits are clearly outweighed by its expected risk. Moreover, in cases where a researcher or institution is unable to comply with federal laws and regulations, or ensure a safe research environment in the context of GOFROC, funding ought not be given for such experiments.. When it comes to the fourth principle-that the research cannot be feasibly or efficaciously pursued through another methodology that answers the same scientific question-it isn't clear that any experiment would fail to satisfy this criterion. Methodologies in the life science closely track the organization of the field, its priorities, and the kinds of question that are asked. In a very simple sense, different methodologies, even if they create similar (or similarly valuable) knowledge, do not ask \"the same question. \" To change methodology is, in a very basic but important sense, to change the question.. This principle highlights, moreover, a strong division in the debate over GOF/PPP research. Advocates for GOF/PPP research have argued that GOF as a methodology has unique epistemic merits. While other experiments may allow us to demonstrate the potential for a pathogen to alter its host range or experience enhanced transmissibility or virulence, advocates maintain that only GOF can show us this is possible. As such, a change to an alternate methodology deprives us of the one methodological tool we have to conclusively prove that a (wild-type) virus can acquire the potential to cause a human pandemic (Casadevall et al., 2014) .. It is thus difficult to understand, then, what work is done by the fourth principle as it is written. One suggested case in which an alternate methodology might answer the same question would be one in which a specific part of a pathogen's structure altered to demonstrate a change in function. 5 Say, for example, we wanted to determine whether a virus with a substituted HA protein would bind to human receptors. It would be possible to answer this question using an attenuated virus rather than its wild-type progenitor. A more radical alternative might be cell-free study of single proteins-for example, H5 or H7 receptor binding to mammalian sialic acids-could take the place of GOF/PPP research with similar aims (Lipsitch and Galvani, 2014) and eliminate the need for a live virus (at least, in initial research). 6 This a rare example, however, and does not represent paradigm GOF/PPP studies studying which mutations in a viral genome are attributed to changes in phenotype under laboratory conditions. Alternate experiments to these GOF/PPP experiments may answer-related questions (Lipsitch and Galvani, 2014) and have considerable scientific and public health value, but do not answer precisely the same question. They are thus excluded as potential replacements for GOF/PPP studies under the new policy.. Yet the existence of alternate experiments, and the serious risks presented by some GOF/PPP research gives us a pro tanto justification for substituting experiments even if they do not ask the same question. This justification is particularly compelling if alternate experiments advance therapeutics development, disease surveillance, or public health response to emerging pandemics. Next, I'll consider three objections that might arise from those who may wish to retain the current formulation of the fourth principle.. Here, I outline four potential objections and respond to them. These are:. 1. Assert the P3CO principles are appropriate in the context of existing scientific governance; 2. Beyond instrumental benefits, scientists should be free to choose which questions they ask; 3. Appeal to the NSABB Recommendations for guidance on \"same or similar\" questions; 4. Appropriately tailoring the risk-benefit difference in principle three will select against the right set of experiments.. I respond to these in turn.. One could contend these P3CO principles are appropriate in the context of existing scientific governance. That is, as long as the expected risk-benefit difference is positive (i.e., sufficiently in favor of benefit), the (scientifically meritorious) research ought to proceed. We ought not be in the business of guessing whether other questions might give equally valuable answers, or trying as policymakers to guess which scientific question is more worthy of pursuing.. Advocates typically justify GOF/PPP experiments on their role in achieving some other end, e.g., developing novel vaccines or therapeutics (Schultz-Cherry et al., 2014), or enhancing disease surveillance (Casadevall et al., 2014) . There is some value in GOF/ PPP research for its own sake, but this is the case for any scientifically meritorious question. Moreover, the value of preventing an accidental or deliberately caused disease pandemic arguably outweighs the mere value of scientific knowledge for its own sake. Advocates of GOF/PPP don't just leverage the instrumental value of these studies in their argument; they rely on them.. Yet funders prioritize those ends, and thus partly determine what scientific questions can be asked; we already do what advocates of GOF/PPP research deny. Allowing similar, but 6 Thanks to an anonymous reviewer for this suggestion. not identical questions to take the place of GOF/PPP research amounts a small change in degree from existing practice. Only occasionally is GOF/PPP research a unique means to medical and public health ends (Evans, 2014) ; other experiments may answer different scientific questions, but still answer questions that achieve the same ends as GOF/PPP research. The GOF benefit analysis conducted during the deliberative process demonstrated that only in 9 of 24 scientific and public health goals addressed for influenza was GOF/PPP uniquely useful; this number was only 3 in 13 for coronavirus (SARS and MERS) research (Gryphon Scientific, 2016, pp. 249-254) .. A proponent of GOF/PPP research might reply that what matters isn't (only) that we achieve our scientific or public health aims, but that scientists are also free to choose which questions they ask. Scientific freedom, after all, is not simply an efficient means to achieve material ends, but valuable for its own sake (Evans, 2013) . This response also fails: scientific freedom is important, but we already acknowledge limits on that freedom given certain risks to others (e.g., in human experimentation). Given that the set of experiments we can permissibly fund-and thus, the questions we ask-is already much smaller than the total set of scientific questions we have available to ask, it seems minimally invasive to ask that where possible, we pick from a set of related questions that accomplish our aims while entailing fewer, or less extreme, risks.. One might appeal to the Recommendations for guidance. In their fourth recommendation, the NSABB notes that review of GOFROC should include consideration of alternate methodologies that provide the \"same or very similar information\" at lower risk. This presumably a larger set than methodologies that strictly answer the same scientific question. How large a set this is, however, depends on what constitutes \"information\" in this sense, and what our basis for comparison determine similarity might be.. What might this conception of similarity look like? Consider an interesting piece of scientific knowledge such as \"the sequence changes required to confer mammalian transmissibility via respiratory droplets onto HPAI viruses (Watanabe et al., 2014) . \" GOF/PPP experiments, at least those that spurred the deliberative processes, ultimately provided one set of information that produces the kind of scientific knowledge we want. It did so, moreover, in a very specific way: by providing the exact sequence change required for specific isolates of HPAI. In the case of the 2011 experiments, these were sequence changes for influenza virus A/Indonesia/5/2005 (Herfst et al., 2012) , and six reassortant viruses from a library of reassortants from A/Vietnam/1203/2004 (H5N1) and A/Puerto Rico/8/34 (H1N1).. Critics have noted that while these experiments provide sequence changes required to confer mammalian transmissibility via respiratory droplets onto HPAI viruses, the sequence changes developed in laboratories may not be likely or even plausible in wild flu viruses (Enserink, 2011; Lipsitch and Galvani, 2014) . Other methodologies may provide likely sequence changes for a broader range of viruses, some (or more) of which are plausible in nature. This provides a similar kind of knowledge, albeit less precise (in the sense that it does not provide us with an exact sequence) but with a greater degree of generalization. Both give us our sought after knowledge in some sense of the initial parameters.. How we understand similarity depends, in part, on what kinds of knowledge we find valuable. Let us assume, plausibly, that the general scientific interest of each study is equal (though perhaps to different sets of scientists). Whether these two sets of experiment provide relevantly \"very similar\" information depends, again, on the relationship between our ends and the kinds of knowledge about which we are interested. If our interest is cataloging individual HPAI viruses for their capacity to undergo sustained transmission and are able to look for those specific sequences in nature, then GOF/PPP experiments occupy a fairly narrow space in which we could find other, similar experiments that generate similar information. If, however, we are interested in determining which kinds of sequence are most likely to appear in flu viruses in the wild, GOF/PPP experiments are arguably not unique and may even by counterproductive as a form of knowledge generation . How we define similarity depends on our ends; P3CO's reduction from similarity to identity commits us to valuing scientific information in a way we might otherwise not endorse.. While I have used the 2011 HPAI H5N1 studies in my argument, other experiments may have different goals for which GOF/PPP methodologies are more suited. Studies involving MERS coronavirus, for example, were allowed to continue after the initial pause because their aim was to develop an animal model for MERS in which to conduct experiments (Kaiser, 2014) . Arguably, GOF/PPP research in aid of this pursuit may contribute uniquely to the development of an animal model in MERS or be so efficacious a method as to outweigh potential risks.. Finally, one might respond that the most crucial aspect here is the risk-benefit difference in principle three. That is, one might argue that the primary principle on which we ought to judge GOF/PPP research is whether its expected benefits outweigh its expected risks. Moreover, if we set our threshold for acceptable risk correctly, we will exclude those experiments that are too risky relative to their expected benefits and default to other methodologies. This is promising insofar as it forces us to specify exactly how much our purported benefits must exceed potential risks in order to justify funding GOF/PPP experiments. Yet it elides the comparative nature of risk assessment. Decisions under conditions of risk are, and ought to be, comparative (see, e.g., Hansson, 2003) . Say we believe the right decision is one that maximizes the probability-weighted net benefits of our actions. And let us also say a GOF/PPP experiment might lead to the development of a novel vaccine: say, a 90% chance, and would save 10 million people; but also entails an independent nontrivial \"global catastrophic risk\" of 0.01% of a disease pandemic that kills 1 billion people. Our probability adjusted net benefit is thus 8.9 million people: we expect, all things considered, that many more people will be saved than die as a result of our experiment. Now imagine an alternate experiment called alt-GOF. This experiment has the same probability of causing benefit or harm, but the magnitude of the benefit and harms are 9.9 million and 10 million, respectively. Our expected value is now 8.909 million lives: about 9,090 more than the initial case. It isn't sufficient to say GOF leads to a positive net benefit. We have clear reason to prefer alt-GOF to GOF/PPP, regardless of whether or not it answers the exact same question. 7 Risk assessments ought to be understood as comparisons rather than absolute references (National Research Council Institute of Medicine, 2015) . It isn't sufficient to say we are justified in funding GOF/PPP research just in case its expected benefits outweigh its expected risks. Rather, we want to say GOF/PPP provides the most justifiable tradeoff between benefit and risk. What that tradeoff ought to be is unclear from the P3CO policy. Regardless of what criteria we use for decision-making under risk, our principles for governance ought to be comparative. Principle four undercuts this by precluding a large set of options from which comparison can follow.. In response to my argument, the following solutions could be implemented individually or in concert.. (1) Clarify principle four in broader terms. HHS and other agencies could release additional guidance that would clarify principle four to include considerations of a reasonable range of alternative experiments with independent scientific merit that promote the same aims as GOF/PPP research. A move away from the \"same question\" or \"very similar information\" to a broader conception of alternatives would develop the governance of this research in important ways, critically in forcing policymakers and scientists to specify what, exactly, they wish GOF/PPP research to accomplish.. This is a plausible in the context of section V of the current HHS policy, which claims that \"HHS will periodically re-evaluate and modify this review process, as necessary, to reflect scientific advances and changes to the regulatory landscape\" (Department of Health and Human Services, 2017). We could envisage such a review tackling, first, the question of alternate experiments and building on existing work [e.g., the risk and benefit assessment from the deliberative process (Gryphon Scientific, 2016) ] to develop specific guidance on alternate experiments that fulfill a range of scientific and public health aims.. (2) Introduce comparative subprinciples into principle three.. That is, rather than attempt to realign principle four with the above critiques, further incorporate comparative risk assessment into the third principle and its translation into actionable policy. By explicitly noting that risk assessments must be comparative over a range of plausible alternatives within the broader aims of the funded research, agencies can side-step potential issues with principle four.. This recommendation would be a plausible alternative to my first recommendation by incorporating the substance of my critique into principle three, rather than principle four. It would conceivably render principle four unnecessary. Rather, the risk assessment would be comparative and account for alternatives that produce valuable scientific information that is both similar to and as valuable as proposed GOF/PPP research, but may entail fewer serious risks. This could be accomplished without a specific policy change in the context of the HHS policy, instead built into future procedural elements of the review process that have yet to be described. The risk here is that excluding principle four as an explicit consideration reopens the possibility that alternates are never taken seriously; I acknowledge this, but note that my above suggestion, taken in good faith, makes this unlikely.. (3) Fund comparative assessments of scientific research and alternatives to GOF/PPP. Requiring a risk and benefit analysis as part of policy is itself challenging, and other policies on dual-use research of concern have been criticized by practitioners for requiring analysis that is beyond the skills of any institutional oversight body. Rather than require de novo analysis for each GOF/PPP experiment to arise, research on these experiments-using the risk and benefit analysis from the deliberative process as a template-could develop a hierarchy of comparatively low-, medium-, and high-risk experiments, and the conditions under which they are uniquely justified or overwhelmingly efficacious. A kind of \"GOF/PPP case law\" is itself a valuable addition to the life sciences, and an important policy innovation.. We already have some precedent for this in the form of those MERS studies that were reinstated after the initial pause. This kind of research arguably provides us with an important data point on what counts as efficacious and justified GOF/PPP research looks like. In contrast, research that created mammalian transmissible variants of a virus that has never been recorded in mammals (Sutton et al., 2014) is arguably a high-risk, unjustified form of GOF/PPP research.. The new GOF/PPP principles require further interpretation to become effective governance. In this article, I have argued that principle four, \"the research cannot be feasibly or efficaciously pursued through another methodology that answers the same scientific question, \" is overly permissive and have suggested reform for policymakers. The debate over GOF/PPP research is surely not over, and whether these changes are ultimately incorporated is a matter for future policy work.. NE responsible for all research and writing.. reFereNces\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE081LKTQgoF"
      },
      "source": [
        "## ***Step 3:*** Tokenize sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtag6JGNRZE8"
      },
      "source": [
        "Here I select to tokenize only the abstract and the body text of each paper, since I wanted to search for the answer in these parts and not in the title. I didn't believe that the answer might be in the title, so I wanted to save more time and memory. Moreover, the titles used to be only one sentence in generall, so it doesn't make much sence to tokenize only one sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zC8wUP2UwIoN"
      },
      "source": [
        "for article in range(len(articles)):\n",
        "  # Tokenize title\n",
        "  # articles[article][0] = sent_tokenize(articles[article][0])\n",
        "\n",
        "  # Tokenize abstract\n",
        "  if articles[article][1] != []:\n",
        "    articles[article][1] = sent_tokenize(articles[article][1])\n",
        "  \n",
        "  # Tokenize body text\n",
        "  articles[article][2] = sent_tokenize(articles[article][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hnrkd8aKrgUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1231bc-9878-4a77-edb4-80c0a154f03e"
      },
      "source": [
        "print(articles[0][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['PPPs) contains a series of principles for governing the funding and conduct of gain-offunction (GOF) research resulting in the creation of PPPs.', 'In this article, I address one of these principles, governing the replacement of GOF research with alternate experiments.', 'I argue that the principle fails to address the way that different experiments can promote the same values as those promoted by GOF research resulting in PPPs.', 'I then address some objections to this claim, and provide policy recommendations moving forward.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjrmGEn7Sozr"
      },
      "source": [
        "## ***Step 4:*** Download the Sentence Transformers library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnwTkg5w8XQo"
      },
      "source": [
        "%%capture\n",
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-aLgdR-YP3-"
      },
      "source": [
        "## ***Step 5:*** Experiment with models of Sentence Transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYZInx9JS6hG"
      },
      "source": [
        "According to the paper ***Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks***, asking BERT to compare sentences is possible but too slow for real-time applications. However, we could pre-compute representations (embeddings) for each sequence in our dataset separate from all other ones. These embeddings could in a second step then be used to measure, for example, similarity using the cosine similarity function which wouldn’t require us to ask BERT to perform this task. SBERT is a so-called twin network which allows it to process two sentences in the same way, simultaneously. SentenceBERT introduces pooling to the token embeddings generated by BERT in order for creating a fixed size sentence embedding. \n",
        "\n",
        "From this paper I see that SBERT has better results from any other word embeddings, such as Bert and Glove Embeddings. For this reason I decide to experiment only with diferrent SBERT models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaaQjEc1enmy"
      },
      "source": [
        "Here, as you can see, I tried the SBERT models nli-roberta-base and stsb-roberta-base. The large nli roberta took several hours and it never ended (because of crashed session). These two models hadn't many diferrence at the given answers, so I decided to keep only the one of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYy9ituKYP4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3520a17d-4bf1-49a7-f0c1-3521ef692823"
      },
      "source": [
        "import sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer,util\n",
        "model_ = SentenceTransformer('nli-roberta-base')\n",
        "# model_ = SentenceTransformer('nli-roberta-large')\n",
        "# model_ = SentenceTransformer('stsb-roberta-base')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 460M/460M [00:06<00:00, 74.5MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLUSSzWxgIxt"
      },
      "source": [
        "Here I took a subset of the whole dataset of 9000 papers, to try different methods, because it takes many hours to run for all the dataset. In the beggining, I experimented with very small datasets until I found the suitable method.\n",
        "\n",
        "So when I find the appropriate strategy, I run all the notebook for the whole dataset of 9000 articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8tkRydUYP4I"
      },
      "source": [
        "import copy\n",
        "\n",
        "mini_articles = articles[:9000]\n",
        "\n",
        "embeddings = copy.deepcopy(mini_articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjFy6ZkOhhC9"
      },
      "source": [
        "## ***Step 6:*** Use GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjAav5kFYP4K",
        "outputId": "8841f6dd-8c55-4662-a22e-36cf0a07cb55"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available()\n",
        "                      else 'cpu')\n",
        "model_.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentenceTransformer(\n",
              "  (0): Transformer(\n",
              "    (auto_model): RobertaModel(\n",
              "      (embeddings): RobertaEmbeddings(\n",
              "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "        (token_type_embeddings): Embedding(1, 768)\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (encoder): RobertaEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (1): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (2): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (3): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (4): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (5): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (6): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (7): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (8): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (9): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (10): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (11): RobertaLayer(\n",
              "            (attention): RobertaAttention(\n",
              "              (self): RobertaSelfAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (output): RobertaSelfOutput(\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): RobertaIntermediate(\n",
              "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            )\n",
              "            (output): RobertaOutput(\n",
              "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (pooler): RobertaPooler(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (activation): Tanh()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (1): Pooling()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpasE459hn5W"
      },
      "source": [
        "## ***Step 7:*** Encode Sentences "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9FiYhkwh2v2"
      },
      "source": [
        "As I said before, I searched for the answer only in abstract and body texts of each paper. For this reason, I concatenate the abstract and the body texts of each paper, so I can encoded them all together. Futhermore, after I encoded to tensors, I kept the original list of text abstarcts and body texts to take the answer in the end.\n",
        "\n",
        "I calculate the time it takes to compute the embeddings and as you can see it takes a lot of time to find the embeddings to the whole dataset. It is a very time consuming procedure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t776owC3IE-s"
      },
      "source": [
        "import time\n",
        "start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQzl3VpYYP4R"
      },
      "source": [
        "origin_list_of_articles = []\n",
        "list_of_articles = []\n",
        "\n",
        "for article in embeddings:\n",
        "  article_list = []\n",
        "  \n",
        "  # Put both abstract and body texts of each article together\n",
        "  if article[1] != []:\n",
        "    for abs_text in article[1]:\n",
        "      article_list.append(abs_text)\n",
        "\n",
        "  for body_text in article[2]:\n",
        "    article_list.append(body_text)\n",
        "\n",
        "  origin_list_of_articles.append(article_list)\n",
        "  # Encode all the article \n",
        "  list_of_articles.append(model_.encode(article_list,convert_to_tensor=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqippEsvIHRc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae0788b-9035-47c6-a789-91dfcca9f505"
      },
      "source": [
        "end_time = time.time()\n",
        "print(end_time-start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3429.3172903060913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ_a4WJrYP4U"
      },
      "source": [
        "# print(list_of_articles[0])\n",
        "# print(len(list_of_articles[0]))\n",
        "# print(origin_list_of_articles[0])\n",
        "# print(origin_list_of_articles[0][0])\n",
        "# print(mini_articles[0][2])\n",
        "# print(len(list_of_articles[0][0]))\n",
        "# print(len(list_of_articles))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJQGYE6bjT3M"
      },
      "source": [
        "## ***Step 8:*** Ask model for answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwOSeO8gjZQw"
      },
      "source": [
        "Here I tried to find the most similar answer of each query, using the measure of ***cosine similarity***. \n",
        "\n",
        "***Cosine Similarity*** calculates similarity by measuring the cosine of angle between two vectors. The cosine similarity is advantageous because even if the two similar documents are far apart by the Euclidean distance (due to the size of the document), chances are they may still be oriented closer together. The smaller the angle, higher the cosine similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_WI6t7WS4Zv"
      },
      "source": [
        "For the search I found out (after I implement the semantic search by myself :) ), that there is already implemented in the library of sentence_transformers.util.\n",
        "\n",
        "***semantic_search*** performs a cosine similarity search between a list of query embeddings and a list of corpus embeddings:\n",
        " \n",
        "- Returns a sorted list with decreasing cosine similarity scores. Entries are dictionaries with the keys ‘corpus_id’ and ‘score’.\n",
        "- I used the parameter top_k  to retrieve ***top 5*** matching entries.\n",
        "\n",
        "In other words, I took each query and I compared it with each sententce of my data, I save the 5 best scores, the indexes of articles and the indexes of the sentences in a list. \n",
        "\n",
        "Finally for each query, I sorted the final list of results to take the 4 most similar sentences in the list. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq2e5k9sbYh_"
      },
      "source": [
        "start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihTM09v3YP4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f70a11-59cb-405f-da80-aaa63c39edd5"
      },
      "source": [
        "# Query sentences:\n",
        "queries = ['What are the coronoviruses?', \n",
        "           'What was discovered in Wuhuan in December 2019?', \n",
        "           'What is Coronovirus Disease 2019?', \n",
        "           'What is COVID-19?', 'What is caused by SARS-COV2?', \n",
        "           'How is COVID-19 spread?', \n",
        "           'Where was COVID-19 discovered?', \n",
        "           'How does coronavirus spread?',\n",
        "           \"Can the use of masks prevent SARS?\",\n",
        "           \"What is the Persistence of virus on surfaces of different materials (e.g. copper, stainless steel, plastic)?\",\n",
        "           \"What ages have higher mortality rate in covid-19?\",\n",
        "           \"What are the protective measures of covid-19?\"]\n",
        "\n",
        "#Encode the queries\n",
        "query_embeddings = model_.encode(queries, convert_to_tensor=True)\n",
        "\n",
        "# Find the closest sentence of the corpus for each query sentence based on cosine similarity\n",
        "for query, query_embedding in zip(queries, query_embeddings):\n",
        " \n",
        "  body_text_distances = []\n",
        "\n",
        "  for idx_of_article,article in enumerate(list_of_articles):\n",
        "    cos_scores = sentence_transformers.util.semantic_search(query_embedding, article, top_k=5)\n",
        "\n",
        "    for elem in cos_scores[0]:\n",
        "      body_text_distances.append((idx_of_article,elem['corpus_id'],elem['score']))\n",
        "\n",
        "  results = sorted(body_text_distances, key=lambda x: x[2], reverse=True)\n",
        "\n",
        "  print(\"\\n=========================================================\\n\")\n",
        "  print(\"Query:\", query)\n",
        "  print(\"\\nThe most similar sentences in corpus:\\n\")\n",
        "\n",
        "  print(\"1. \",origin_list_of_articles[results[0][0]][results[0][1]], \"(Score: %.4f)\" % (results[0][2]))  \n",
        "  print(\"From title: \", mini_articles[results[0][0]][0])\n",
        "\n",
        "  print('\\n')\n",
        "  print(\"2. \",origin_list_of_articles[results[1][0]][results[1][1]], \"(Score: %.4f)\" % (results[1][2]))  \n",
        "  print(\"From title: \", mini_articles[results[1][0]][0])\n",
        "\n",
        "  print('\\n')\n",
        "  print(\"3. \",origin_list_of_articles[results[2][0]][results[2][1]], \"(Score: %.4f)\" % (results[2][2]))  \n",
        "  print(\"From title: \", mini_articles[results[2][0]][0])\n",
        "\n",
        "  print('\\n')\n",
        "  print(\"4. \",origin_list_of_articles[results[3][0]][results[3][1]], \"(Score: %.4f)\" % (results[3][2]))  \n",
        "  print(\"From title: \", mini_articles[results[3][0]][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=========================================================\n",
            "\n",
            "Query: What are the coronoviruses?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  Coronaviruses. (Score: 0.8747)\n",
            "From title:  DV-Curve Representation of Protein Sequences and Its Application\n",
            "\n",
            "\n",
            "2.  The Coronaviridae Family. (Score: 0.8689)\n",
            "From title:  A Genome-Wide Analysis of RNA Pseudoknots That Stimulate Efficient −1 Ribosomal Frameshifting or Readthrough in Animal Viruses\n",
            "\n",
            "\n",
            "3.  Coccidioides spp. (Score: 0.8634)\n",
            "From title:  Clinical Practice and Cases\n",
            "\n",
            "\n",
            "4.  2 | Phylogeny of coronaviruses. (Score: 0.8594)\n",
            "From title:  Consensus statement The species Severe acute respiratory syndrome- related coronavirus: classifying 2019-nCoV and naming it SARS-CoV-2 Coronaviridae Study Group of the International Committee on Taxonomy of Viruses*\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Query: What was discovered in Wuhuan in December 2019?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  In December 2019 (Score: 0.9152)\n",
            "From title:  Clinical Medicine Characteristics of and Public Health Responses to the Coronavirus Disease 2019 Outbreak in China\n",
            "\n",
            "\n",
            "2.  The case time series data in December 2019 were obtained from a published study [3] . (Score: 0.8738)\n",
            "From title:  Clinical Medicine Estimating the Unreported Number of Novel Coronavirus (2019-nCoV) Cases in China in the First Half of January 2020: A Data-Driven Modelling Analysis of the Early Outbreak\n",
            "\n",
            "\n",
            "3.  SARS-2 started in Wuhan in Hubei Province no later than early December 2019 [7] . (Score: 0.8608)\n",
            "From title:  Potential Factors Influencing Repeated SARS Outbreaks in China\n",
            "\n",
            "\n",
            "4.  Novel Coronavirus (2019-nCoV) is an emerging pathogen that was first identified in Wuhan, China in late December 2019. (Score: 0.8401)\n",
            "From title:  pathogens Emergence of Novel Coronavirus 2019-nCoV: Need for Rapid Vaccine and Biologics Development\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Query: What is Coronovirus Disease 2019?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  gov/coronavirus/2019-ncov/index.html).. (Score: 0.8837)\n",
            "From title:  Morbidity and Mortality Weekly Report\n",
            "\n",
            "\n",
            "2.  The 2019 novel coronavirus (2019-nCoV), a betacoronavirus, forms a clade within the subgenus sarbecovirus of the Orthocoronavirinae subfamily [4] . (Score: 0.8830)\n",
            "From title:  Potential Rapid Diagnostics, Vaccine and Therapeutics for 2019 Novel Coronavirus (2019-nCoV): A Systematic Review\n",
            "\n",
            "\n",
            "3.  Following the name given by the World Health Organization (WHO), we tentatively call it novel coronavirus 2019 (2019-nCoV). (Score: 0.8760)\n",
            "From title:  A pneumonia outbreak associated with a new coronavirus of probable bat origin\n",
            "\n",
            "\n",
            "4.  gov/coronavirus/2019-nCoV/hcp/clinical-criteria.html. (Score: 0.8701)\n",
            "From title:  Morbidity and Mortality Weekly Report\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Query: What is COVID-19?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  (IM19). (Score: 0.9131)\n",
            "From title:  Chinese immigrant parents' vaccination decision making for children: a qualitative analysis\n",
            "\n",
            "\n",
            "2.  (DOCX) S19 Information. (Score: 0.9048)\n",
            "From title:  Multiplexed Component Analysis to Identify Genes Contributing to the Immune Response during Acute SIV Infection\n",
            "\n",
            "\n",
            "3.  1.19) . (Score: 0.8980)\n",
            "From title:  Differential transcriptional responses to Ebola and Marburg virus infection in bat and human cells OPEN\n",
            "\n",
            "\n",
            "4.  19 ). (Score: 0.8971)\n",
            "From title:  NAR Breakthrough Article A novel role for poly(C) binding proteins in programmed ribosomal frameshifting\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Query: What is caused by SARS-COV2?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  Step 2: For the diagnosis of Plasmodium spp. (Score: 0.8546)\n",
            "From title:  Title: Causes of fever in Gabonese children: a cross-sectional hospital-based study METHODS Laboratory analyses Laboratory investigations were conducted on-site at CERMEL's laboratories in\n",
            "\n",
            "\n",
            "2.  In the case of rNDV/IBV-S2 strain ( Fig. (Score: 0.8513)\n",
            "From title:  A Recombinant Newcastle Disease Virus (NDV) Expressing S Protein of Infectious Bronchitis Virus (IBV) Protects Chickens against IBV and NDV OPEN\n",
            "\n",
            "\n",
            "3.  2A) and SARS-CoV (Fig. (Score: 0.8450)\n",
            "From title:  These Viruses in Cell Culture\n",
            "\n",
            "\n",
            "4.  SARS-associated Coronavirus (SARS-CoV) has been identified as the causative agent [2] . (Score: 0.8387)\n",
            "From title:  Respiratory Research Persistence of lung inflammation and lung cytokines with high-resolution CT abnormalities during recovery from SARS\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Query: How is COVID-19 spread?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  As a control plasmid pUC19 was used. (Score: 0.8272)\n",
            "From title:  The Ubiquitin Proteasome System Plays a Role in Venezuelan Equine Encephalitis Virus Infection\n",
            "\n",
            "\n",
            "2.  Based on the abov allpiq (19) and:. (Score: 0.8094)\n",
            "From title:  molecules iPPBS-Opt: A Sequence-Based Ensemble Classifier for Identifying Protein-Protein Binding Sites by Optimizing Imbalanced Training Datasets\n",
            "\n",
            "\n",
            "3.  This corresponds to our observations concerning the RespiFinder-19. (Score: 0.8035)\n",
            "From title:  Comparison of three multiplex PCR assays for the detection of respiratory viral infections: evaluation of xTAG respiratory virus panel fast assay, RespiFinder 19 assay and RespiFinder SMART 22 assay\n",
            "\n",
            "\n",
            "4.  It is in the following mentioned as PCA19.. (Score: 0.8027)\n",
            "From title:  Immunome Research Analysis and prediction of protective continuous B-cell epitopes on pathogen proteins\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Query: Where was COVID-19 discovered?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  LE19P directs the synthesis of a de novo initiated 19-nt product. (Score: 0.8323)\n",
            "From title:  Subgenomic promoter recognition by the norovirus RNA-dependent RNA polymerases\n",
            "\n",
            "\n",
            "2.  [18] [19] [20] What has been found? (Score: 0.8269)\n",
            "From title:  Cohort Profile: The Flu Watch Study\n",
            "\n",
            "\n",
            "3.  The proposed FPASSA starts by receiving the historical COVID-19 dataset. (Score: 0.8218)\n",
            "From title:  Clinical Medicine Optimization Method for Forecasting Confirmed Cases of COVID-19 in China\n",
            "\n",
            "\n",
            "4.  Statistics were performed with SPSS-19. (Score: 0.8135)\n",
            "From title:  P1 Cerebral autoregulation testing in a porcine model of intravenously administrated E. coli induced fulminant sepsis\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Query: How does coronavirus spread?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  (8) How do chaperone networks participate in the morphogenesis of reovirus particles? (Score: 0.8361)\n",
            "From title:  viruses Function, Architecture, and Biogenesis of Reovirus Replication Neoorganelles\n",
            "\n",
            "\n",
            "2.  a Depiction of the case history surrounding myeloablation and coronavirus infection. (Score: 0.8342)\n",
            "From title:  CASE REPORT Myeloablation-associated deletion of ORF4 in a human coronavirus 229E infection\n",
            "\n",
            "\n",
            "3.  Using the PICO format (acronym for \"population or problem\", intervention or exposure of interest\", \"comparison\" and \"outcome\") [25], the research questions were: \"what is the frequency of chronic comorbidities in flavivirus infections?\" (Score: 0.8217)\n",
            "From title:  Prevalence of chronic comorbidities in dengue fever and West Nile virus: A systematic review and meta-analysis\n",
            "\n",
            "\n",
            "4.  The assay eEvaluates virus replication and production of infectious progeny. (Score: 0.8153)\n",
            "From title:  Synthetic sulfonated derivatives of poly (allylamine hydrochloride) as inhibitors of human metapneumovirus\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Query: Can the use of masks prevent SARS?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  This result implies that HCWs could use appropriate personal protective measures (such as avoiding face-to-face contact with SARS patients) to protect themselves from SARS infection while they are caring for patients with SARS.. (Score: 0.8679)\n",
            "From title:  BMC Public Health Which preventive measures might protect health care workers from SARS?\n",
            "\n",
            "\n",
            "2.  † If possible, a SARS patient should wear a surgical mask during close contact with uninfected persons. (Score: 0.8663)\n",
            "From title:  The Search for a Practical Approach to Emerging Diseases: The Case of Severe Acute Respiratory Syndrome (SARS)\n",
            "\n",
            "\n",
            "3.  ), and protective measures (wearing masks, gowns and goggles when in contact with SARS patients). (Score: 0.8307)\n",
            "From title:  BMC Infectious Diseases Association of SARS susceptibility with single nucleic acid polymorphisms of OAS1 and MxA genes: a case-control study\n",
            "\n",
            "\n",
            "4.  LF may have potential therapeutic applications as a drug candidate for the treatment of SARS disease.. (Score: 0.8274)\n",
            "From title:  Inhibition of SARS Pseudovirus Cell Entry by Lactoferrin Binding to Heparan Sulfate Proteoglycans\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Query: What is the Persistence of virus on surfaces of different materials (e.g. copper, stainless steel, plastic)?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  Do elements of the cytoskeleton or other cellular components play a specific role in mediating envelopment or budding of filamentous or spherical virus particles? (Score: 0.8520)\n",
            "From title:  Electron Tomography Reveals the Steps in Filovirus Budding\n",
            "\n",
            "\n",
            "2.  Pictured are the stages of virus infection in which interplay between viruses and host lipids occur. (Score: 0.8482)\n",
            "From title:  Lipid interactions during virus entry and infection\n",
            "\n",
            "\n",
            "3.  For both constructs, characteristic SARS-CoV-induced cytopathic effect (CPE) was observed in electroporated cells, and culture medium contained infectious virus as determined by plaque assay. (Score: 0.8480)\n",
            "From title:  Infidelity of SARS-CoV Nsp14-Exonuclease Mutant Virus Replication Is Revealed by Complete Genome Sequencing\n",
            "\n",
            "\n",
            "4.  Considering that genome replication of Reoviridae viruses occurs inside viral cores (28, 29) , the significance of membrane fragments labeled with anti-dsRNA or anti-BrU antibodies inside VIs is uncertain. (Score: 0.8438)\n",
            "From title:  Reovirus NS and NS Proteins Remodel the Endoplasmic Reticulum to Build Replication Neo-Organelles\n",
            "\n",
            "=========================================================\n",
            "\n",
            "Query: What ages have higher mortality rate in covid-19?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  However, older respondents estimated a higher death toll. (Score: 0.8728)\n",
            "From title:  The dynamics of risk perceptions and precautionary behavior in response to 2009 (H1N1) pandemic influenza\n",
            "\n",
            "\n",
            "2.  The SOFA score-based definition of sepsis predicted mortality higher than that of SIRS-based definition. (Score: 0.8608)\n",
            "From title:  Procalcitonin as a prognostic marker for sepsis based on SEPSIS-3\n",
            "\n",
            "\n",
            "3.  Mortality rates was higher in AKI group (p = 0, 01). (Score: 0.8590)\n",
            "From title:  \n",
            "\n",
            "\n",
            "4.  Older age (≥ 64) was significantly associated with higher mortality rate in univariate analysis (HR = 1.54 (1.26-1.88), p < 0.0001). (Score: 0.8582)\n",
            "From title:  \n",
            "\n",
            "=========================================================\n",
            "\n",
            "Query: What are the protective measures of covid-19?\n",
            "\n",
            "The most similar sentences in corpus:\n",
            "\n",
            "1.  As a control plasmid pUC19 was used. (Score: 0.8740)\n",
            "From title:  The Ubiquitin Proteasome System Plays a Role in Venezuelan Equine Encephalitis Virus Infection\n",
            "\n",
            "\n",
            "2.  This corresponds to our observations concerning the RespiFinder-19. (Score: 0.8617)\n",
            "From title:  Comparison of three multiplex PCR assays for the detection of respiratory viral infections: evaluation of xTAG respiratory virus panel fast assay, RespiFinder 19 assay and RespiFinder SMART 22 assay\n",
            "\n",
            "\n",
            "3.  Figure 19 : Our method. (Score: 0.8516)\n",
            "From title:  Alignment-free method for DNA sequence clustering using Fuzzy integral similarity ROC_supplementary material Explanation\n",
            "\n",
            "\n",
            "4.  ENC was calculated using the following formula [19] :. (Score: 0.8447)\n",
            "From title:  Comprehensive Analysis and Comparison on the Codon Usage Pattern of Whole Mycobacterium tuberculosis Coding Genome from Different Area\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acaGCQYcbbUw",
        "outputId": "3f0817c2-8916-4685-9b95-84c598412687"
      },
      "source": [
        "end_time = time.time()\n",
        "print(end_time-start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71.13007307052612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGPeyrZoo5bB"
      },
      "source": [
        "## ***Step 9:*** Summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_-4r0FLkw5m"
      },
      "source": [
        "- As you can see the results isn't so satisfactory and I waited one hour to take these results. I thought that the Brute Force Method, to compare the queries with each sentence of the paper, would give me the right answer, but unfortunately this wasn't happened. \n",
        "\n",
        "- In the beginning, I print only the first most similar sentence and the results were heartbreaking. It was funny (or nor) to make a question of \"What was discovered in Wuhuan in December 2019?\" and the given answer was \"In December 2019\". The score of similarity is very good (0.9152) and we can understand the reason. The results aren't irrational, but there also aren't and the right answer. However if we look more carefully we will see that the title of the answer isn't so bad: \"Clinical Medicine Characteristics of and Public Health Responses to the Coronavirus Disease 2019 Outbreak in China\".\n",
        "\n",
        "- For this reason, I decided to show the top 4 similar sentences, to give a chance to see if there were better results with less score, but more relevant with the meaning of the question. As we see, in some questions the results seems to be better like \"SARS-2 started in Wuhan in Hubei Province no later than early December 2019.\" or \"Novel Coronavirus (2019-nCoV) is an emerging pathogen that was first identified in Wuhan, China in late December 2019.\" which is the right answer to the previous question.\n",
        "\n",
        "- However, there are still questions that the results remain unsatisfied, such as \"What is COVID-19?\".\n",
        "\n",
        "- It is very logic to me to have this answers as the best, because for a question that includes the word of COVID-19, the cosine similarity will perform better in documents that have the number 19 inside them. For example the answers of \"(IM19)\" or \"(DOCX) S19 Information\" or \"1.19).\" or \"19 ).\" in the question \"What is COVID-19?\", isn't so unexpected. Futhermore, these results are because of the sentence tokenize that seperates the sentences where there is a period \".\". Maybe if I show more similar answers, there would be a right answer eventually. So this was a chance for me to thing something better than only cosine similarity.\n",
        "\n",
        "- As I was tried to load subsequently datasets, I didn't see any different in my answers, but only in time, because they have more articles.\n",
        "\n",
        "- I have to mention that I added some of my own questions to see the results like \"What ages have higher mortality rate in covid-19?\" , whose almost all of the answers are right. Another questions that I give are \"Can the use of masks prevent SARS?\" or \"What are the protective measures of covid-19?\" or \"What is the Persistence of virus on surfaces of different materials (e.g., copper, stainless steel, plastic)?\" that they didn't go very well.\n",
        "\n",
        "- As for the pretrained nli-roberta-base that I chose, it was slightly slower than stsb-roberta-base' (~1 second slower for each article), but because I didn't saw any basic difference in results, I kept the nli-roberta-base pretrained model, as according to the paper it performs better results.\n",
        "\n",
        "- I have to refer that I tried to take with cosine similarity, the top most similar titles, and then for these 100 articles I took the top 50 most similar abstracts and finally the 20 most similar text bodies. However the results was even worse and for this reason I decided to not present them to you (it is the code of comments below). Besides the search of the answer didn't take so much time (only ~1 minute), to have a reason to reduce data. The time consuming procedure is to encode the sentences of each data.\n",
        "\n",
        "- So I decided to not encode all the dataset for the answer to my second model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22fR3Pohowvv"
      },
      "source": [
        "# # Query sentences:\n",
        "# queries = ['What are the coronoviruses?', \n",
        "#            'What was discovered in Wuhuan in December 2019?', \n",
        "#            'What is Coronovirus Disease 2019?', \n",
        "#            'What is COVID-19?', 'What is caused by SARS-COV2?', \n",
        "#            'How is COVID-19 spread?', \n",
        "#            'Where was COVID-19 discovered?', \n",
        "#            'How does coronavirus spread?']\n",
        "\n",
        "# query_embeddings = model.encode(queries)\n",
        "\n",
        "# # Find the closest sentence of the corpus for each query sentence based on cosine similarity\n",
        "# for query, query_embedding in zip(queries, query_embeddings):\n",
        " \n",
        "#   title_distances = []\n",
        "#   for article in range(len(mini_articles)):\n",
        "#     title_distances.append(scipy.spatial.distance.cdist([query_embedding], [embeddings[article][0]], \"cosine\")[0])\n",
        "\n",
        "#   results = zip(range(len(title_distances)), title_distances)\n",
        "#   results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "#   top = 100\n",
        "#   abs_distances = []\n",
        "#   for idx, distance in results[0:top]:\n",
        "#     sum = 0\n",
        "#     num_of_sentences = 0\n",
        "#     for sentence in range(len(mini_articles[idx][1])):\n",
        "#       sum += scipy.spatial.distance.cdist([query_embedding], [embeddings[idx][1][sentence]], \"cosine\")[0]\n",
        "#       num_of_sentences += 1\n",
        "#     if num_of_sentences != 0:\n",
        "#       avg = sum/num_of_sentences\n",
        "#     abs_distances.append((idx,avg))\n",
        "\n",
        "#   results = sorted(abs_distances, key=lambda x: x[1])\n",
        "\n",
        "#   top = 50\n",
        "#   body_text_distances = []\n",
        "  \n",
        "#   for idx, distance in results[0:top]:\n",
        "#     distances = []\n",
        "#     num_of_sentence = 0\n",
        "#     for sentence in range(len(embeddings[idx][2])):\n",
        "#       distances.append((num_of_sentence, (scipy.spatial.distance.cdist([query_embedding], [embeddings[idx][2][sentence]], \"cosine\")[0])))\n",
        "#       num_of_sentence += 1\n",
        "    \n",
        "#     results = sorted(distances, key=lambda x: x[1])\n",
        "\n",
        "#     body_text_distances.append((idx,results[0][0], results[0][1]))\n",
        "\n",
        "#   results = sorted(body_text_distances, key=lambda x: x[2])\n",
        "\n",
        "#   print(\"\\n======================\\n\")\n",
        "#   print(\"Query:\", query)\n",
        "#   print(\"\\nThe most similar sentence in corpus:\\n\")\n",
        "\n",
        "#   print(origin_list_of_articles[results[0][0]][results[0][1]], \"(Score: %.4f)\" % (1-results[0][2]))  "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}